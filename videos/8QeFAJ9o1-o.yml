# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata

tags:
    - unix
    - bigdata
    - ml
    - architecture
    - python
    - lisp
    - scala
    - java
title: 'Jim Baker - Scalable Realtime Architectures in Python'
recordingDate: 1411368816
description: "Jim Baker - Scalable Realtime Architectures in Python\n[EuroPython 2014]\n[25 July 2014]\n\nThis talk will focus on you can readily implement highly scalable and fault tolerant realtime architectures, such as dashboards, using Python and tools like Storm, Kafka, and ZooKeeper. We will focus on two related aspects: composing reliable systems using at-least-once and idempotence semantics and how to partition for locality.\n\n-----\n\nIncreasingly we are interested in implementing highly scalable and\nfault tolerant realtime architectures such as the following:\n\n* Realtime aggregation. This is the realtime analogue of working with\n  batched map-reduce in systems like Hadoop.\n\n* Realtime dashboards. Continuously updated views on all your\n  customers, systems, and the like, without breaking a sweat.\n\n* Realtime decision making. Given a set of input streams, policy on\n  what you like to do, and models learned by machine learning, optimize a\n  business process. One example includes autoscaling a set of servers.\n\n(We use realtime in the soft sense: systems that are continuously\ncomputing on input streams of data and make a best effort to keep up;\nit certainly does not imply hard realtime systems that strictly\nbound their computation times.)\n\nObvious tooling for such implementations include Storm (for event\nprocessing), Kafka (for queueing), and ZooKeeper (for tracking and\nconfiguration). Such components, written respectively in Clojure\n(Storm), Scala (Kafka), and Java (ZooKeeper), provide the desired\nscalability and reliability. But what may not be so obvious at first\nglance is that we can work with other languages, including Python, for\nthe application level of such architectures. (If so inclined, you can\nalso try reimplementing such components in Python, but why not use\nsomething that's been proven to be robust?)\n\nIn fact Python is likely a better language for the app level, given\nthat it is concise, high level, dynamically typed, and has great\nlibraries. Not to mention fun to write code in! This is especially\ntrue when we consider the types of tasks we need to write: they are\nvery much like the data transformations and analyses we would have\nwritten of say a standard Unix pipeline. And no one is going to argue\nthat writing such a filter in say Java is fun, concise, or even\nconsiderably faster in running time.\n\nSo let's look at how you might solve such larger problems. Given that\nit was straightforward to solve a small problem, we might approach as\nfollows. Simply divide up larger problems in small one. For example,\nperhaps work with one customer at a time. And if failure is an ever\npresent reality, then simply ensure your code retries, just like you\nmight have re-run your pipeline against some input files.\n\nUnfortunately both require distributed coordination at scale. And\ndistributed coordination is challenging, especially for real systems,\nthat will break at scale. Just putting a box in your architecture\nlabeled **\"ZooKeeper\"** doesn't magically solve things, even if\nZooKeeper can be a very helpful part of an actual solution.\n\nEnter the Storm framework. While Storm certainly doesn't solve all\nproblems in this space, it can support many different types of\nrealtime architectures and works well with Python. In particular,\nStorm solves two key problems for you.\n\n**Partitioning**. Storm lets you partition streams, so you can break\ndown the size of your problem. But if the a node running your code\nfails, Storm will restart it. Storm also ensures such topology\ninvariants as the number of nodes (spouts and bolts in Storm's lingo)\nthat are running, making it very easy to recover from such failures.\n\nThis is where the cleverness really begins. What can you do if you can\nensure that **all the data** you need for a given continuously updated\ncomputation - what is the state of this customer's account?  - can be\nput in **exactly one place**, then flow the supporting data through it\nover time? We will look at how you can readily use such locality in\nyour own Python code.\n\n**Retries**. Storm tracks success and failure of events being\nprocessed efficiently through a batching scheme and other\ncleverness. Your code can then choose to retry as necessary. Although\nStorm also supports exactly-once event processing semantics, we will\nfocus on the simpler model of at-least-once semantics. This means your\ncode must tolerate retry, or in a word, is idempotent. But this is\nstraightforward. We have often written code like the following:\n\n    seen = set()\n    for record in stream:\n        k = uniquifier(record)\n        if k not in seen:\n           seen.add(k)\n           process(record)\n\nExcept of course that any such real usage has to ensure it doesn't\nattempt to store all observations (first, download the Internet! ;),\nbut removes them by implementing some sort of window or uses data\nstructures like HyperLogLog, as we will discuss."
